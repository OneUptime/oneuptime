global:
  storageClass:
  clusterDomain: &global-cluster-domain cluster.local


# Please change this to the domain name / IP where OneUptime server is hosted on.
host: localhost
httpProtocol: http

# Important: You do need to set this to a long random values if you're using OneUptime in production.
oneuptimeSecret:
encryptionSecret:

# External Secrets
# You need to leave blank oneuptimeSecret and encryptionSecret to use this section
externalSecrets:
  oneuptimeSecret:
    existingSecret:
      name:
      passwordKey:
  encryptionSecret:
    existingSecret:
      name:
      passwordKey:

# (Optional): You usually do not need to set this if you're self hosting.
openTelemetryCollectorHost:
fluentdHost:

deployment:
  # Default replica count for all deployments
  replicaCount: 1 

metalLb:
  enabled: false
  ipAdddressPool:
    enabled: false
    addresses:
      # - 51.158.55.153/32 # List of IP addresses of all the servers in the cluster.

nginx:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  listenAddress: ""
  listenOptions: ""
  serverNamesHash:
    bucketSize: ""
    maxSize: ""
  ports:
    http: 80
    https: 443
  service:
    loadBalancerIP:
    type: LoadBalancer
    externalIPs:
      # - 51.158.55.153 # Please make sure this is the same as the one in metalLb.ipAdddressPool.addresses



postgresql:
  enabled: true # Set this to false if you're using an external postgresql database.
  image:
    repository: postgres
    tag: latest
    pullPolicy: IfNotPresent
  auth:
    database: oneuptimedb
    # Username is fixed to "postgres"
    # Will be auto-generated if not provided
    postgresPassword: 
  primary:
    service:
      type: ClusterIP
      ports:
        postgresql: "5432"
    terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
    persistence:
      enabled: true
      size: 25Gi
      storageClass: ""
    nodeSelector: {}
    tolerations: []
    affinity: {}
  # Optional: override global security contexts just for the PostgreSQL pod/container
  # podSecurityContext:
  #   runAsUser: 999
  #   runAsGroup: 999
  #   fsGroup: 999
  # containerSecurityContext:
  #   allowPrivilegeEscalation: false
  #   readOnlyRootFilesystem: true
  #   capabilities:
  #     drop: ["ALL"]
    resources: {}
    configuration: |-
      # Network and connection settings
      listen_addresses = '*'
      port = 5432
      # Core tuning (adjust as needed)
      max_connections = 100
      shared_buffers = 128MB
    # pg_hba.conf rules. These enable password auth (md5) from any host/IP.
    # Tighten these for production to your pod/service/network CIDRs.
    hbaConfiguration: |-
      # Allow all IPv4 and IPv6 clients with md5 password auth
      host    all             all             0.0.0.0/0               md5
      host    all             all             ::/0                    md5

  

clickhouse:
  enabled: true
  auth:
    username: oneuptime
    # Will be auto-generated if not provided  
    password:
  image:
    repository: clickhouse/clickhouse-server
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    ports:
      http: "8123"
      tcp: "9000"
      mysql: "9004"
      postgresql: "9005"
      interserver: "9009"
  persistence:
    enabled: true
    size: 25Gi
    storageClass: ""
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Optional: override global security contexts just for the ClickHouse pod/container
  # podSecurityContext:
  #   runAsUser: 101
  #   runAsGroup: 101
  #   fsGroup: 101
  # containerSecurityContext:
  #   allowPrivilegeEscalation: false
  #   readOnlyRootFilesystem: true
  #   capabilities:
  #     drop: ["ALL"]
  resources: {}
  # Custom ClickHouse configuration
  configuration: |-
    <clickhouse>
        <logger>
            <level>warning</level>
            <console>true</console>
        </logger>
        <http_port>8123</http_port>
        <tcp_port>9000</tcp_port>
        <mysql_port>9004</mysql_port>
        <postgresql_port>9005</postgresql_port>
        <interserver_http_port>9009</interserver_http_port>
        <listen_host>::</listen_host>
        <users>
            <${CLICKHOUSE_USER}>
                <password>${CLICKHOUSE_PASSWORD}</password>
                <networks>
                    <ip>::/0</ip>
                </networks>
                <profile>default</profile>
                <quota>default</quota>
                <access_management>1</access_management>
            </${CLICKHOUSE_USER}>
        </users>
        <quotas>
            <default>
                <interval>
                    <duration>3600</duration>
                    <queries>0</queries>
                    <errors>0</errors>
                    <result_rows>0</result_rows>
                    <read_rows>0</read_rows>
                    <execution_time>0</execution_time>
                </interval>
            </default>
        </quotas>
    </clickhouse>


redis:
  enabled: true
  auth:
    # Will be auto-generated if not provided
    password: 
  image:
    repository: redis
    tag: latest
    pullPolicy: IfNotPresent
  master:
    service:
      type: ClusterIP
      ports:
        redis: "6379"
    persistence:
      enabled: false 
      size: 8Gi
      storageClass: ""
    nodeSelector: {}
    tolerations: []
    affinity: {}
  # Optional: override global security contexts just for the Redis pod/container
  # podSecurityContext:
  #   runAsUser: 999
  #   runAsGroup: 999
  #   fsGroup: 999
  # containerSecurityContext:
  #   allowPrivilegeEscalation: false
  #   readOnlyRootFilesystem: true
  #   capabilities:
  #     drop: ["ALL"]
    resources: {}
  commonConfiguration: |-
   appendonly no
   save ""


image:
  registry: docker.io
  repository: oneuptime
  pullPolicy: Always
  tag: release
  restartPolicy: Always
# imagePullSecrets:
#   - name: MyCustomSecretNameWithDockerCredentials

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeEnvironment: production

billing:
  enabled: false
  publicKey:
  privateKey:
  smsDefaultValueInCents:
  callDefaultValueInCentsPerMinute:
  smsHighRiskValueInCents:
  callHighRiskValueInCentsPerMinute:
  allowedActiveMonitorCountInFreePlan: 10

subscriptionPlan:
  basic:
  growth:
  scale:
  enterprise:

analytics:
  host:
  key:

# VAPID Configuration for Web Push Notifications
# Generate VAPID keys using: npx web-push generate-vapid-keys
vapid:
  publicKey:
  privateKey:
  subject: mailto:support@oneuptime.com

incidents:
  disableAutomaticCreation: false

alerts:
  disableAutomaticCreation: false

# If you would like to attach status page to custom domains use this setting.
# For example, lets say you would like the status page to be hosted on status.yourcompany.com, then
# 1. Create a A record in your DNS provider with the name "oneuptime.yourcompany.com" and value to Public IP of the server oneuptime is deployed on.
# 2. Set the statusPage.cnameRecord to "oneuptime.yourcompany.com"
# 3. Create CNAME record in your DNS provider with the name "status.yourcompany.com" and value "oneuptime.yourcompany.com"
statusPage:
  replicaCount: 1
  cnameRecord:
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3105

probes:
  one:
    name: "Probe"
    description: "Probe"
    monitoringWorkers: 3
    monitorFetchLimit: 10
    monitorRetryLimit: 3
    key:
    replicaCount: 1
    syntheticMonitorScriptTimeoutInMs: 60000
    customCodeMonitorScriptTimeoutInMs: 60000
    disableTelemetryCollection: false
    disableAutoscaler: false
    ports:
      http: 3874
    # Proxy configuration for probe connections
    proxy:
      # HTTP proxy URL for HTTP requests (optional)
      # Format: http://[username:password@]proxy.server.com:port
      # Example: http://proxy.example.com:8080
      # Example with auth: http://username:password@proxy.example.com:8080
      httpProxyUrl:
      # HTTPS proxy URL for HTTPS requests (optional)
      # Format: http://[username:password@]proxy.server.com:port
      # Example: http://proxy.example.com:8080
      # Example with auth: http://username:password@proxy.example.com:8080
      httpsProxyUrl:
    # KEDA autoscaling configuration based on monitor queue metrics
    keda:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      # Scale up when queue size exceeds this threshold per probe
      queueSizeThreshold: 10
      # Polling interval for metrics (in seconds)
      pollingInterval: 30
      # Cooldown period after scaling (in seconds)
      cooldownPeriod: 300
#   resources:
#   additionalContainers:
# two:
#   name: "Probe 2"
#   description: "Probe 2"
#   monitoringWorkers: 3
#   monitorFetchLimit: 10
#   key:
#   replicaCount: 1
#   ports:
#     http: 3874
#   syntheticMonitorScriptTimeoutInMs: 60000
#   customCodeMonitorScriptTimeoutInMs: 60000
#   disableTelemetryCollection: false
#   disableAutoscaler: false
#   # Proxy configuration for probe connections
#   proxy:
#     # HTTP proxy URL for HTTP requests (optional)
#     httpProxyUrl:
#     # HTTPS proxy URL for HTTPS requests (optional) 
#     httpsProxyUrl:
#   resources:
#   additionalContainers:
    # KEDA autoscaling configuration based on monitor queue metrics
    # keda:
    #   enabled: false
    #   minReplicas: 1
    #   maxReplicas: 100
    #   # Scale up when queue size exceeds this threshold per probe
    #   queueSizeThreshold: 10
    #   # Polling interval for metrics (in seconds)
    #   pollingInterval: 30
    #   # Cooldown period after scaling (in seconds)
    #   cooldownPeriod: 300


testServer:
  replicaCount: 1
  enabled: false
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3800


openTelemetryExporter:
  endpoint:
  # This can be for example: x-oneuptime-token=<YOUR_ONEUPTIME_TELEMETRY_INGEST_TOKEN>
  headers:

containerSecurityContext:
podSecurityContext:
affinity:
tolerations:
nodeSelector:


# This can be one of the following: DEBUG, INFO, WARN, ERROR, OFF
# Please do not set this to DEBUG in production. This is only for development / debugging purposes.
logLevel: INFO

# Enable cleanup cron jobs
# Please do not enable this in production. This is only for development purposes.
cronJobs:
  cleanup:
    enabled: false
  e2e:
    # Please do not enable this in production. This is only for development purposes.
    enabled: false
    isUserRegistered: false
    registeredUserEmail:
    registeredUserPassword:
    # This is the URL of the status page you want to test. This is used to check if the status page is up and running.
    statusPageUrl:
    failedWebhookUrl:


# Please add this information if you want to generate SSL certificates for your status page custom domains. 
# This is only required if you're using custom domains for status pages.
letsEncrypt:
  # Generate a random private key via openssl, encode it to base64
  # Example: "LS0tLS....1cbg=="
  accountKey:
  # Email address to register with letsencrypt for notifications
  # Example: "hello@yourcompany.com"
  email:

oneuptimeIngress:
  enabled: false
  annotations:
  # Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  # If you dont have nginx ingress controller installed, please install it by going to https://kubernetes.github.io/ingress-nginx/deploy/
  className: nginx # Required. Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  hosts:  # List of hosts for the ingress. Please change this to your hosts
    # - "oneuptime.com" # Host 1
    # - "www.oneuptime.com" # Host 2
  tls:
    enabled: false
    hosts:
      # - host: "oneuptime.com" # Host 1
      #   secretName: "oneuptime-tls"  # This secret will be created by Cert-Manager if enabled


script:
  workflowScriptTimeoutInMs: 5000

# extraTemplates -- Array of extra objects to deploy with the release. Strings
# are evaluated as a template and can use template expansions and functions. All
# other objects are used as yaml.
extraTemplates:
  #- |
  #    apiVersion: v1
  #    kind: ConfigMap
  #    metadata:
  #      name: my-configmap
  #    data:
  #      key: {{ .Values.myCustomValue | quote }}


# External Postgres Configuration
# You need to set postgresql.enabled to false if you're using an external postgres database.
externalPostgres:
  host:
  port:
  username:
  password:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  ssl:
    enabled: false
    # If this is enabled, please set either "ca"
    ca:
    # (optional)
    cert:
    key:

## External Redis Configuration
# You need to set redis.enabled to false if you're using an external redis database.

externalRedis:
  host:
  port:
  username:
  password:
  ipFamily:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  tls:
    enabled: false
    # If this is enabled, please set "ca" certificate.
    ca:
    # (optional)
    cert:
    key:


## External Clickhouse Configuration
# You need to set clickhouse.enabled to false if you're using an external clickhouse database.
externalClickhouse:
  host:
  ## If the host is https, set this to true. Otherwise, set it to false.
  isHostHttps: false
  port:
  username:
  password:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  tls:
    enabled: false
    # If this is enabled, please set either "ca"
    ca:
    # (optional)
    cert:
    key:


# Notification webhooks when certain events happen in the system. (usually they are slack webhooks)
notifications:
  webhooks:
    slack: 
      # This is the webhook that will be called when a user is created or signs up.
      onCreateUser:
      onDeleteProject: 
      onCreateProject: 
      onSubscriptionUpdate: 



startupProbe: # Startup probe configuration
  enabled: true
  periodSeconds: 60
  failureThreshold: 18

livenessProbe: # Liveness probe configuration
  enabled: true
  periodSeconds: 60
  timeoutSeconds: 120
  initialDelaySeconds: 10

readinessProbe: # Readiness probe configuration
  enabled: true
  periodSeconds: 60
  initialDelaySeconds: 10
  timeoutSeconds: 120


# OpenTelemetry Collector Configuration
openTelemetryCollector: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    grpc: 4317
    http: 4318
  sendingQueue: 
    enabled: true
    size: 1000
    numConsumers: 3

accounts: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3003
  resources:

home: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1444
  resources:

dashboard:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3009
  resources:

adminDashboard:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3158
  resources:

worker:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 1445
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

workflow:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  workflowTimeoutInMs: 5000
  ports:
    http: 3099
  resources:

apiReference:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1446
  resources:

docs:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1447
  resources:

app: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3002
  resources:

probeIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3400
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

openTelemetryIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  # Max concurrent telemetry jobs processed by each pod
  concurrency: 100
  ports:
    http: 3403
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

fluentIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3401
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

incomingRequestIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3402
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

isolatedVM:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 4572
  resources:

serverMonitorIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3404
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300


slackApp: 
  clientId:
  clientSecret:
  signingSecret:


keda:
  enabled: true

cert-manager:
  enabled: false
  installCRDs: true

# Let's Encrypt configuration for Cert-Manager
certManagerLetsEncrypt:
  # Please enable cert-manager.enabled before enabing this to ensure cert-manager is installed and all CRDs are created.
  enabled: false
  email: ""  # Set your email for Let's Encrypt notifications
  server: "https://acme-v02.api.letsencrypt.org/directory"  # Use "https://acme-staging-v02.api.letsencrypt.org/directory" for staging