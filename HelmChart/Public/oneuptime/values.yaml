global:
  storageClass:
  clusterDomain: &global-cluster-domain cluster.local

# Please change this to the domain name / IP where OneUptime server is hosted on.
host: localhost
httpProtocol: http

# Important: You do need to set this to a long random values if you're using OneUptime in production.
oneuptimeSecret:
encryptionSecret:

# External Secrets
# You need to leave blank oneuptimeSecret and encryptionSecret to use this section
externalSecrets:
  oneuptimeSecret:
    existingSecret:
      name:
      passwordKey:
  encryptionSecret:
    existingSecret:
      name:
      passwordKey:

# (Optional): You usually do not need to set this if you're self hosting.
openTelemetryCollectorHost:
fluentdHost:

deployment:
  # Default replica count for all deployments
  replicaCount: 1 

metalLb:
  enabled: false
  ipAdddressPool:
    enabled: false
    addresses:
      # - 51.158.55.153/32 # List of IP addresses of all the servers in the cluster.

nginx:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  listenAddress: ""
  listenOptions: ""
  serverNamesHash:
    bucketSize: ""
    maxSize: ""
  ports:
    http: 80
    https: 443
  service:
    loadBalancerIP:
    type: LoadBalancer
    externalIPs:
      # - 51.158.55.153 # Please make sure this is the same as the one in metalLb.ipAdddressPool.addresses

postgresql:
  enabled: true # Set this to false if you're using an external postgresql database.
  clusterDomain: *global-cluster-domain
  auth:
    username: oneuptime
    database: oneuptimedb
  architecture: standalone
  primary:
    service:
      ports:
         postgresql: "5432"
    terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
    persistence:
      size: 25Gi
  readReplicas:
    terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
    persistence:
      size: 25Gi

clickhouse:
  enabled: true
  clusterDomain: *global-cluster-domain
  keeper:
    enabled: false
  service:
    ports:
      http: "8123"
      tcp: "9000"
      mysql: "9004"
      postgresql: "9005"
  shards: 1
  replicaCount: 1
  terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
  zookeeper:
    enabled: false
  persistence:
    size: 25Gi
  auth:
    username: oneuptime
  # For some reason bitnami clickhouse choose to have resourcesPreset: "small" as default. We dont want that.
  resourcesPreset: "none" 
  configdFiles:
    99-disable-db-interface.xml: |-
      <clickhouse>
        <mysql_port remove='1'></mysql_port>
        <postgresql_port remove='1'></postgresql_port>
        <tcp_port remove='1'></tcp_port>
      </clickhouse>
  initdbScripts:
    db-init.sql: |
      CREATE DATABASE oneuptime;


redis:
  enabled: true
  clusterDomain: *global-cluster-domain
  architecture: standalone
  ipFamily: 4
  auth:
    enabled: true
  master:
    service:
      ports:
        redis: "6379"
    persistence:
      enabled: false # We dont need redis persistence, because we dont do anything with it.
  replica:
    persistence:
      enabled: false # We dont need redis persistence, because we dont do anything with it.
  commonConfiguration: |-
   appendonly no
   save ""


image:
  registry: docker.io
  repository: oneuptime
  pullPolicy: Always
  tag: release
  restartPolicy: Always
# imagePullSecrets:
#   - name: MyCustomSecretNameWithDockerCredentials

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeEnvironment: production

billing:
  enabled: false
  publicKey:
  privateKey:
  smsDefaultValueInCents:
  callDefaultValueInCentsPerMinute:
  smsHighRiskValueInCents:
  callHighRiskValueInCentsPerMinute:
  allowedActiveMonitorCountInFreePlan: 10

subscriptionPlan:
  basic:
  growth:
  scale:
  enterprise:

analytics:
  host:
  key:

# VAPID Configuration for Web Push Notifications
# Generate VAPID keys using: npx web-push generate-vapid-keys
vapid:
  publicKey:
  privateKey:
  subject: mailto:support@oneuptime.com

incidents:
  disableAutomaticCreation: false

alerts:
  disableAutomaticCreation: false

# If you would like to attach status page to custom domains use this setting.
# For example, lets say you would like the status page to be hosted on status.yourcompany.com, then
# 1. Create a A record in your DNS provider with the name "oneuptime.yourcompany.com" and value to Public IP of the server oneuptime is deployed on.
# 2. Set the statusPage.cnameRecord to "oneuptime.yourcompany.com"
# 3. Create CNAME record in your DNS provider with the name "status.yourcompany.com" and value "oneuptime.yourcompany.com"
statusPage:
  replicaCount: 1
  cnameRecord:
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3105

probes:
  one:
    name: "Probe"
    description: "Probe"
    monitoringWorkers: 3
    monitorFetchLimit: 10
    monitorRetryLimit: 3
    key:
    replicaCount: 1
    syntheticMonitorScriptTimeoutInMs: 60000
    customCodeMonitorScriptTimeoutInMs: 60000
    disableTelemetryCollection: false
    disableAutoscaler: false
    ports:
      http: 3874
    # KEDA autoscaling configuration based on monitor queue metrics
    keda:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      # Scale up when queue size exceeds this threshold per probe
      queueSizeThreshold: 10
      # Polling interval for metrics (in seconds)
      pollingInterval: 30
      # Cooldown period after scaling (in seconds)
      cooldownPeriod: 300
#   resources:
#   additionalContainers:
# two:
#   name: "Probe 2"
#   description: "Probe 2"
#   monitoringWorkers: 3
#   monitorFetchLimit: 10
#   key:
#   replicaCount: 1
#   ports:
#     http: 3874
#   syntheticMonitorScriptTimeoutInMs: 60000
#   customCodeMonitorScriptTimeoutInMs: 60000
#   disableTelemetryCollection: false
#   disableAutoscaler: false
#   resources:
#   additionalContainers:
    # KEDA autoscaling configuration based on monitor queue metrics
    # keda:
    #   enabled: false
    #   minReplicas: 1
    #   maxReplicas: 100
    #   # Scale up when queue size exceeds this threshold per probe
    #   queueSizeThreshold: 10
    #   # Polling interval for metrics (in seconds)
    #   pollingInterval: 30
    #   # Cooldown period after scaling (in seconds)
    #   cooldownPeriod: 300


testServer:
  replicaCount: 1
  enabled: false
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3800


openTelemetryExporter:
  endpoint:
  # This can be for example: x-oneuptime-token=<YOUR_ONEUPTIME_TELEMETRY_INGEST_TOKEN>
  headers:

containerSecurityContext:
podSecurityContext:
affinity:
tolerations:
nodeSelector:


# This can be one of the following: DEBUG, INFO, WARN, ERROR, OFF
# Please do not set this to DEBUG in production. This is only for development / debugging purposes.
logLevel: INFO

# Enable cleanup cron jobs
# Please do not enable this in production. This is only for development purposes.
cronJobs:
  cleanup:
    enabled: false
  e2e:
    # Please do not enable this in production. This is only for development purposes.
    enabled: false
    isUserRegistered: false
    registeredUserEmail:
    registeredUserPassword:
    # This is the URL of the status page you want to test. This is used to check if the status page is up and running.
    statusPageUrl:
    failedWebhookUrl:


# Please add this information if you want to generate SSL certificates for your status page custom domains. 
# This is only required if you're using custom domains for status pages.
letsEncrypt:
  # Generate a random private key via openssl, encode it to base64
  # Example: "LS0tLS....1cbg=="
  accountKey:
  # Email address to register with letsencrypt for notifications
  # Example: "hello@yourcompany.com"
  email:

oneuptimeIngress:
  enabled: false
  annotations:
  # Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  # If you dont have nginx ingress controller installed, please install it by going to https://kubernetes.github.io/ingress-nginx/deploy/
  className: nginx # Required. Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  hosts:  # List of hosts for the ingress. Please change this to your hosts
    # - "oneuptime.com" # Host 1
    # - "www.oneuptime.com" # Host 2
  tls:
    enabled: false
    hosts:
      # - host: "oneuptime.com" # Host 1
      #   secretName: "oneuptime-tls


script:
  workflowScriptTimeoutInMs: 5000

# extraTemplates -- Array of extra objects to deploy with the release. Strings
# are evaluated as a template and can use template expansions and functions. All
# other objects are used as yaml.
extraTemplates:
  #- |
  #    apiVersion: v1
  #    kind: ConfigMap
  #    metadata:
  #      name: my-configmap
  #    data:
  #      key: {{ .Values.myCustomValue | quote }}


# External Postgres Configuration
# You need to set postgresql.enabled to false if you're using an external postgres database.
externalPostgres:
  host:
  port:
  username:
  password:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  ssl:
    enabled: false
    # If this is enabled, please set either "ca"
    ca:
    # (optional)
    cert:
    key:

## External Redis Configuration
# You need to set redis.enabled to false if you're using an external redis database.

externalRedis:
  host:
  port:
  username:
  password:
  ipFamily:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  tls:
    enabled: false
    # If this is enabled, please set "ca" certificate.
    ca:
    # (optional)
    cert:
    key:


## External Clickhouse Configuration
# You need to set clickhouse.enabled to false if you're using an external clickhouse database.
externalClickhouse:
  host:
  ## If the host is https, set this to true. Otherwise, set it to false.
  isHostHttps: false
  port:
  username:
  password:
  # If you're using an existing secret for the password, please use this instead of password.
  existingSecret:
    name:
    # This is the key in the secret where the password is stored.
    passwordKey:
  database:
  tls:
    enabled: false
    # If this is enabled, please set either "ca"
    ca:
    # (optional)
    cert:
    key:


# Notification webhooks when certain events happen in the system. (usually they are slack webhooks)
notifications:
  webhooks:
    slack: 
      # This is the webhook that will be called when a user is created or signs up.
      onCreateUser:
      onDeleteProject: 
      onCreateProject: 
      onSubscriptionUpdate: 



startupProbe: # Startup probe configuration
  enabled: true
  periodSeconds: 60
  failureThreshold: 18

livenessProbe: # Liveness probe configuration
  enabled: true
  periodSeconds: 60
  timeoutSeconds: 120
  initialDelaySeconds: 10

readinessProbe: # Readiness probe configuration
  enabled: true
  periodSeconds: 60
  initialDelaySeconds: 10
  timeoutSeconds: 120


# OpenTelemetry Collector Configuration
openTelemetryCollector: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    grpc: 4317
    http: 4318
  sendingQueue: 
    enabled: true
    size: 1000
    numConsumers: 3

accounts: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3003
  resources:

home: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1444
  resources:

dashboard:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3009
  resources:

adminDashboard:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3158
  resources:

worker:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 1445
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

workflow:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  workflowTimeoutInMs: 5000
  ports:
    http: 3099
  resources:

apiReference:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1446
  resources:

docs:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 1447
  resources:

app: 
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 3002
  resources:

probeIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3400
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

openTelemetryIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  # Max concurrent telemetry jobs processed by each pod
  concurrency: 100
  ports:
    http: 3403
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

fluentIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3401
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

incomingRequestIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3402
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300

isolatedVM:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  ports:
    http: 4572
  resources:

serverMonitorIngest:
  replicaCount: 1
  disableTelemetryCollection: false
  disableAutoscaler: false
  concurrency: 100
  ports:
    http: 3404
  resources:
  # KEDA autoscaling configuration based on queue metrics
  keda:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    # Scale up when queue size exceeds this threshold
    queueSizeThreshold: 100
    # Polling interval for metrics (in seconds)
    pollingInterval: 30
    # Cooldown period after scaling (in seconds)
    cooldownPeriod: 300


slackApp: 
  clientId:
  clientSecret:
  signingSecret:

microsoftTeamsApp:
  appId:
  appPassword:
  tenantId:

keda:
  enabled: true